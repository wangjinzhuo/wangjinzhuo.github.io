<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[Collaborative learing with knowledge sharing]]></title>
      <url>/2017/06/07/cll-mkb/</url>
      <content type="html"><![CDATA[<p>Recently arise many successful deep architectures that are carefully designed for tasks in artificial intelligence, computer vision, natural language processing and speech recognition. In such cases, one model (or deep architecture) trained with one task (specific input and output) can learn corresponding knowledge. Different models can learn task-specific knowledge. On the other hand, the target tasks share many relevant properties (such as image classification and semantic segmentation), the learnt knowledge that exists among the model is expected to have something in common. This paper aim at levering such information to achieve model assistance, and consequently improving the performance of involved models.</p>
<a id="more"></a>
<p>In particular, we introduce a new learnable module called collaborative learning layer (CLL) that is responsible to mix the learnt knowledge of involved models and extract relevant knowledge to achieve assistance. The core structure of our proposed module is built around a differentiable mutual knowledge base (MKB), which preserves and updates the mutual knowledge.  The MKB recieves encoded knowledge of individual models and mixed them into a universal representation. We show how to train CLL along with involved models and the use of CLL can improve the performance of involved models.</p>
<p>The CLL is very flexible and can be deployed with many settings. First, it can be inserted among multiple architectures, which means it is able to help model assistance. Second, multiple CLLs can be used to enhance the performance of Collaborative learning. Third, CLL can be applied with tasks that are the same, relevant, and even non-relevant.</p>
]]></content>
      
        
        <tags>
            
            <tag> Deep learning </tag>
            
            <tag> On-going project </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Properties of neural networks]]></title>
      <url>/2017/04/07/properties-of-neural-networks/</url>
      <content type="html"><![CDATA[<p>Intriguing properties of neural networks – <a href="https://arxiv.org/pdf/1312.6199.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1312.6199.pdf</a></p>
<ol>
<li>It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks.</li>
<li>It suggests that the learnt input-output mappings by deep neural networks are fairly discontinuous. That means it is easy to fool a well trained neural networks.</li>
</ol>
<a id="more"></a>
<p>See also in <a href="https://arxiv.org/pdf/1412.6572.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1412.6572.pdf</a> where the authors show it is easy to generate adversarial examples which are close to the original ones but are misclassified by neural networks. </p>
]]></content>
      
        
        <tags>
            
            <tag> Deep learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Conference attendance report of AAAI-2017]]></title>
      <url>/2017/02/19/aaai-2017/</url>
      <content type="html"><![CDATA[<p>AAAI 2017 took place in San Francisco and lasted for 5 days.</p>
<p>First time attending to AAAI, a coverall AI conference which accepts papers of every aspect of AI research.</p>
<a id="more"></a>
<p>A great talk given by Prof. Grauman about learning from unlabeled videos <a href="http://www.cs.utexas.edu/%7Egrauman/slides/grauman-AAAI2017.pdf" target="_blank" rel="external">http://www.cs.utexas.edu/%7Egrauman/slides/grauman-AAAI2017.pdf</a>, which I think is close to recent popular attention model.<br><img src="/images/aaai.jpg" alt=""></p>
]]></content>
      
        
        <tags>
            
            <tag> Conference attendance </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Conference attendance report of NIPS 2016]]></title>
      <url>/2016/12/15/nips-2016/</url>
      <content type="html"><![CDATA[<p>Glad to attend NIPS 2016 in Barcelona, a beautiful costal city in Spain. It is lucky for a football fan to join a party in Nou Camp.</p>
<p>There are many famous researchers including LeCun, Schmidhuber and other yound research scientists in DeepMind and OpenAI. They give talks, introduce posts, communicate and share ideas. I like such atmosphere.</p>
<a id="more"></a>
<p>An impressive work is “Learning to learn by gradient descent by gradient descent” by M.Andrychowicz et.al. in DeepMind. I read its previous version in arxiv and asked the authors at poster room in the first evening. You can not imagine how popular the DeepMind’s paper was. I waited for almost half of an hour to ask the author several questions. </p>
<p><img src="/images/learning.png" alt=""></p>
]]></content>
      
        
        <tags>
            
            <tag> Conference attendance </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Smartcity competition]]></title>
      <url>/2015/07/21/anomaly-detection-match/</url>
      <content type="html"><![CDATA[<p>The finals of Smartcity competition for chinese graduates raised the curtain in Wuhan University, July.</p>
<p>Our team participated in the task of video anomaly detection. Our algorithm is based on an unsupervised analysis of trajectory features of normal videos, as the number of anomaly videos available is rather small. We first build a mix model composed of normal bases, and calculate the distance of test videos to the learnt bases to make judgement.</p>
<a id="more"></a>
<p>Our final performance ranked at the second position. It is a not bad result.</p>
<p><img src="/images/wuhan.jpg" alt=""></p>
]]></content>
      
        
        <tags>
            
            <tag> Video anomaly detection </tag>
            
            <tag> competition </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
